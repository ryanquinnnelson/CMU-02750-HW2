{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moving-suggestion",
   "metadata": {},
   "source": [
    "# Question 2: DH algorithm (50 points)\n",
    "**In this question we are going to implement the DH algorithm according to the paper https://icml.cc/Conferences/2008/papers/324.pdf and try to predict protein localization sites in Eukaryotic cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-calendar",
   "metadata": {},
   "source": [
    "### Wald's Approximation\n",
    "\n",
    "$$\\hat{p}_{v,l} \\pm z_{\\alpha}\\sqrt{\\left( \\frac{\\hat{p}_{v,l}\\left( 1-\\hat{p}_{v,l} \\right)}{n} \\right)} $$\n",
    "\n",
    "where \n",
    "- $z_{\\alpha}$ is the $\\left( 1-\\frac{\\alpha}{2}\\right)$ percentile of the standard normal distribution $N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-combine",
   "metadata": {},
   "source": [
    "### Imports and Setup\n",
    "**Note: To use the Neural Network classifier, you need to install [pytorch](https://pytorch.org/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "perfect-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# for model selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# custom packages\n",
    "import packages.dh.helper as helper\n",
    "import packages.dh.dh as dh\n",
    "\n",
    "# setup\n",
    "seed = 2021\n",
    "warnings. filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-understanding",
   "metadata": {},
   "source": [
    "## Part 2.0 Data loading and hierarchical clustering\n",
    "**The DH algorithm is based on hierarchical clustering of the dataset. We will use the DH algorithm on this classification problem: [Protein Localization Prediction](https://archive.ics.uci.edu/ml/datasets/Yeast).**\n",
    "\n",
    "**The first step is to load the dataset and conduct a hierarchical clustring using the `scipy` package. This part has been implemented, read through the code to make sure you understand what is being done.**\n",
    "\n",
    "**NOTES:**\n",
    "- **X_train: data matrix 538x8**\n",
    "- **Y_train: true labels 538x1**\n",
    "- **X_test: data matrix 135x8**\n",
    "- **Y_test: true labels 135x1**\n",
    "\n",
    "**TIPS:**\n",
    "- **Check out this [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) for details about hierarchical clustering.**\n",
    "- **If you are unfamiliar with hierarchical clustering using scipy, [this](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ ) is another helpful resource. (We won't use dendrograms here, but he gives a nice explanation of how to interpret the linkage matrix).**\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-output",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-complex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>NUC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1    X2    X3    X4   X5   X6    X7    X8 Label\n",
       "0  0.58  0.61  0.47  0.13  0.5  0.0  0.48  0.22   MIT\n",
       "1  0.43  0.67  0.48  0.27  0.5  0.0  0.53  0.22   MIT\n",
       "2  0.64  0.62  0.49  0.15  0.5  0.0  0.53  0.22   MIT\n",
       "3  0.58  0.44  0.57  0.13  0.5  0.0  0.54  0.22   NUC\n",
       "4  0.42  0.44  0.48  0.54  0.5  0.0  0.48  0.22   MIT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv('data/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "characteristic-conviction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['MIT', 'NUC'], dtype=object), array([244, 429]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.Label, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-mongolia",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smart-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(538, 8)\n",
      "(538,)\n",
      "(135, 8)\n",
      "(135,)\n"
     ]
    }
   ],
   "source": [
    "# code moved into custom Python module dh.helper\n",
    "# read data\n",
    "df = pd.read_csv('data/data.csv')\n",
    "\n",
    "# filter out samples which are not in filter_class\n",
    "filter_class = ['MIT', 'NUC']\n",
    "mask = df.Label == 0\n",
    "for x in filter_class:\n",
    "    mask = mask | (df.Label == x)\n",
    "df = df[mask]\n",
    "\n",
    "# extract DataFrame features\n",
    "X = df.iloc[:, :8].to_numpy()\n",
    "\n",
    "# extract DataFrame labels and encode labels\n",
    "y = df.Label.astype('category').cat.codes.to_numpy()\n",
    "\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-baghdad",
   "metadata": {},
   "source": [
    "### Construct hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "declared-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 3],\n",
       "        [1, 2],\n",
       "        [4, 5]]),\n",
       " array([1., 1., 1., 1., 2., 2., 4.]),\n",
       " {6: 0, 0: 4, 3: 4, 1: 5, 2: 5, 4: 6, 5: 6}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code moved into custom Python module dh.helper\n",
    "# toy example\n",
    "T = helper.generate_T(X_train[:4])\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broke-copying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dendrogram to visualize clustering\n",
    "Z = linkage(X_train[:4], method='ward')\n",
    "dn = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-trinity",
   "metadata": {},
   "source": [
    "# Part 2.0.1 Supervised classification methods.\n",
    "**We provide several classifiers that can be used. Choose your favourite one. The classifier is going to be used in 2.2, the choose of classifier won't influence your grade.**\n",
    "\n",
    "**TODO:**\n",
    "- **Choose and initialize a classifier**\n",
    "\n",
    "**Note: To use the Neural Network classifier, you need to install [pytorch](https://pytorch.org/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understanding-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(choice, X=None, y=None, seed=None):\n",
    "    \"\"\"\n",
    "    Generates a classifier of the specified choice.\n",
    "\n",
    "    :param choice: String representing the desired classifier\n",
    "    :param X: Used to define Neural Network\n",
    "    :param y: Used to define Neural Network\n",
    "    :param seed: Used to define Random Forest, Gradient Boosting, Neural Network\n",
    "    :return: initialized model\n",
    "    \"\"\"\n",
    "    model = None\n",
    "\n",
    "    if choice == 'Logistic Regression':\n",
    "        lr = LogisticRegression()\n",
    "        model = lr\n",
    "\n",
    "    elif choice == 'Random Forest':\n",
    "        N_estimator_rf = 20\n",
    "        MAX_depth_rf = 6\n",
    "        rf = RandomForestClassifier(n_estimators=N_estimator_rf,\n",
    "                                    max_depth=MAX_depth_rf,\n",
    "                                    random_state=seed)\n",
    "        model = rf\n",
    "\n",
    "    elif choice == 'Gradient Boosting Decision Tree':\n",
    "        N_estimator_gbdt = 20\n",
    "        gbdt_max_depth = 6\n",
    "        gbdt = GradientBoostingClassifier(n_estimators=N_estimator_gbdt,\n",
    "                                          learning_rate=0.1,\n",
    "                                          max_depth=gbdt_max_depth,\n",
    "                                          random_state=seed)\n",
    "        model = gbdt\n",
    "\n",
    "    elif choice == 'Neural Net':\n",
    "        \n",
    "        import torch\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        from torch import optim\n",
    "\n",
    "        # 3-layer fully connected neural network\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        class NNClassifier(object):\n",
    "            def __init__(self,\n",
    "                         feature_n,\n",
    "                         class_n,\n",
    "                         hidden_n=30,\n",
    "                         learning_rate=4e-3,\n",
    "                         weight_decay=1e-5):\n",
    "                self.model = torch.nn.Sequential(torch.nn.Linear(feature_n, hidden_n),\n",
    "                                                 torch.nn.SiLU(),\n",
    "                                                 torch.nn.Linear(hidden_n, hidden_n),\n",
    "                                                 torch.nn.SiLU(),\n",
    "                                                 torch.nn.Linear(hidden_n, class_n))\n",
    "                self.lr = learning_rate\n",
    "                self.wd = weight_decay\n",
    "\n",
    "            def fit(self, X_train, y_train, epoches=300, batch_size=50):\n",
    "                X_t = torch.from_numpy(X_train.astype(np.float32))\n",
    "                y_t = torch.from_numpy(y_train.astype(np.int64))\n",
    "                dataset = TensorDataset(X_t, y_t)\n",
    "                loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "                loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "                optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "                loss_record = 0.0\n",
    "                report_epoch = 50\n",
    "                for epoch_i in range(epoches):\n",
    "                    for batch in loader:\n",
    "                        x_batch, y_batch = batch\n",
    "                        y_pred = self.model(x_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "                        self.model.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        loss_record += loss.item()\n",
    "                    if epoch_i % report_epoch == report_epoch - 1:\n",
    "                        # print(\"[%d|%d] epoch loss:%.2f\" % (epoch_i + 1, epoches, loss_record / report_epoch))\n",
    "                        loss_record = 0.0\n",
    "                    if epoch_i >= epoches:\n",
    "                        break\n",
    "                return self\n",
    "\n",
    "            def score(self, X_test, y_test):\n",
    "                X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "                y_pred_test = self.model(X_test_tensor)\n",
    "                y_output = torch.argmax(y_pred_test, axis=1).numpy()\n",
    "                return (y_output == y_test).mean()\n",
    "\n",
    "        nn = NNClassifier(feature_n=X.shape[1], class_n=len(np.unique(y)))\n",
    "        model = nn\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-naples",
   "metadata": {},
   "source": [
    "### Comparison of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "primary-glory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression: \t\t0.874\n",
      "Accuracy of random forest: \t\t\t0.874\n",
      "Accuracy of Gradient Boosting Decision Tree: \t0.867\n",
      "Accuracy of Neural Network: \t\t\t0.889\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "lr = get_classifier('Logistic Regression').fit(X_train,y_train)\n",
    "rf = get_classifier('Random Forest', seed).fit(X_train,y_train)\n",
    "gbdt = get_classifier('Gradient Boosting Decision Tree', seed).fit(X_train,y_train)\n",
    "nn = get_classifier('Neural Net', X_train,y_train, seed).fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Accuracy of 4 classifiers.\n",
    "print('Accuracy of logistic regression: \\t\\t{:.3f}'.format(lr.score(X_test,y_test)))\n",
    "print('Accuracy of random forest: \\t\\t\\t{:.3f}'.format(rf.score(X_test,y_test)))\n",
    "print('Accuracy of Gradient Boosting Decision Tree: \\t{:.3f}'.format(gbdt.score(X_test,y_test)))\n",
    "print('Accuracy of Neural Network: \\t\\t\\t{:.3f}'.format(nn.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-passion",
   "metadata": {},
   "source": [
    "### Choose and initialize your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "present-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier('Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-dialogue",
   "metadata": {},
   "source": [
    "## Part 2.1 Implement DH algorithm (Hierarchical Sampling for Active Learning). (30 points)\n",
    "\n",
    "**TODO:**\n",
    "- **Please complete the functions to implement the DH algorithm and run the active learning algorithm on the training dataset.**\n",
    "- **The utils functions has been implemented and attached in the homework folder, including `update_empirical.py`, `best_pruning_and_labeling.py`, `assign_labels.py`, and `get_leaves.py`. Please read them and finish the following functions to implement the DH algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "yellow-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved all helper functions into module dh.helper\n",
    "# implemented in custom Python package dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-lighter",
   "metadata": {},
   "source": [
    "## Part 2.2 Run the sample code (10 points)\n",
    "**TODO:**\n",
    "- **Run the following sample code and compare the two figures.**\n",
    "- **Answer the following questions about the results:**\n",
    "    - **How fast does each model converge?**\n",
    "    - **How does error change as we apply more iterations?**\n",
    "    - **Which selection strategy is more accurate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "similar-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10) # control size of plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "transsexual-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? why make a copy of classifier when only one can be used in the function?\n",
    "def call_DH(part, clf, budget, X_train, y_train, X_test, y_test, choice):\n",
    "    \"\"\"\n",
    "    Main function to run all your code once complete.  After you complete\n",
    "   select_case_1() and select_case_2(), this will run the DH algorithm for each\n",
    "   dataset and generate the plots you will submit within your write-up.\n",
    "\n",
    "    :param part: which part of the homework to run\n",
    "    :param clf: The classifier to be trained on the dataset.\n",
    "    :param budget: The number of times that one can query a label from the oracle.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_trials = 5\n",
    "    batch_size = 10\n",
    "#     clf2 = copy.deepcopy(clf)  # unnecessary\n",
    "    axs = plt.subplot()\n",
    "\n",
    "    # initialize components\n",
    "    T = helper.generate_T(X_train)\n",
    "    loss = np.zeros(budget)  # loss for each iteration\n",
    "    \n",
    "    # choose action depending on part\n",
    "    if part.lower() == \"b\":\n",
    "\n",
    "        print(\"Running part B \" + choice + \"...\")\n",
    "\n",
    "        # run trials\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}...\".format(i))\n",
    "            L, error = dh.select_case_1(X_train, y_train, T, budget, batch_size)\n",
    "            loss += error\n",
    "        loss /= num_trials\n",
    "\n",
    "\n",
    "        # train the classifier clf on the predicted label.\n",
    "        clf.fit(X_train, L[:len(X_train)])\n",
    "\n",
    "        # plot\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print('Accuracy of classifier trained on proportional sampling dataset: \\t{:.3f}'.format(score))\n",
    "        axs.plot(np.arange(budget), loss, label=\"Proportional sampling \" + choice)\n",
    "\n",
    "    elif part.lower() == \"c\":\n",
    "\n",
    "        print(\"Running part C \" + choice + \"...\")\n",
    "\n",
    "        # run trials\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}...\".format(i))\n",
    "            L, error = dh.select_case_2(X_train, y_train, T, budget, batch_size)\n",
    "            loss += error\n",
    "        loss /= num_trials\n",
    "\n",
    "        # train the classifier clf on the predicted label\n",
    "        clf.fit(X_train, L[:len(X_train)])\n",
    "\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print('Accuracy of classifier trained on confidence adjusted sampling dataset: \\t{:.3f}'.format(score))\n",
    "        axs.plot(np.arange(budget), loss, label=\"Confidence adjusted sampling \" + choice)\n",
    "\n",
    "    else:\n",
    "        print(\"Incorrect part argument. Either 'b', 'c', 'd', or 'e' expected:\", part)\n",
    "\n",
    "    \n",
    "    axs.set_ylim([0, 0.5])\n",
    "    axs.set_xlabel(\"Number of query samples\")\n",
    "    axs.set_ylabel(\"Error rate\")\n",
    "    plt.title('Number of Queries vs Error Rate')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig(\"img/q2_2_\" + part.lower() + \"_budget\" + str(budget) + \"_\" + str(batch_size) + \"_\" + choice + \".png\")\n",
    "    print()\n",
    "    return loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-terry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running part B Random Forest...\n",
      "Currently on iteration 0...\n",
      "Currently on iteration 1...\n",
      "Currently on iteration 2...\n",
      "Currently on iteration 3...\n",
      "Currently on iteration 4...\n",
      "Accuracy of classifier trained on proportional sampling dataset: \t0.874\n",
      "\n",
      "Running part C Random Forest...\n",
      "Currently on iteration 0...\n",
      "Currently on iteration 1...\n",
      "Currently on iteration 2...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 9 min for all one method with 200 budget\n",
    "\n",
    "# choices= ['Logistic Regression','Random Forest', 'Gradient Boosting Decision Tree','Neural Net']\n",
    "choices = ['Random Forest']\n",
    "losses = []\n",
    "\n",
    "# run active learning\n",
    "BUDGET = 200\n",
    "for part in \"bc\":\n",
    "    for choice in choices:\n",
    "        classifier = get_classifier(choice, X_train,y_train, seed)\n",
    "        loss = call_DH(part,classifier,BUDGET,X_train, y_train, X_test, y_test, choice)\n",
    "        losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-lawrence",
   "metadata": {},
   "source": [
    "### Answers to questions about model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-reggae",
   "metadata": {},
   "source": [
    "#### How fast does each model converge?\n",
    "The proportional (random) sampling model and confidence adjusted (active learning) sampling converge at about the same rate. Within only a few queries, both models achieve an error rate 20-30%. Afterward, they improve (approximately) linearly with the number of queries.\n",
    "\n",
    "#### How does error change as we apply more iterations?\n",
    "For both models, error rate decreases (approximately) linearly within the first 200 queries. The error rate reduces at a faster rate the more queries are performed for the active learning model.\n",
    "\n",
    "#### Which selection strategy is more accurate?\n",
    "For the first 100 queries, both models have similar accuracy. After 100 queries, the active learning model decreases its error rate more quickly. This indicates that the active learning algorithm is more accurate overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-wrestling",
   "metadata": {},
   "source": [
    "### Comparison with Offline Learner\n",
    "For fun. Offline learner receives randomly selected points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "random_error = []\n",
    "\n",
    "X_train_rand = copy.deepcopy(X_train)\n",
    "y_train_rand = copy.deepcopy(y_train)\n",
    "\n",
    "for t in range(200):\n",
    "    \n",
    "    # select sample from pool randomly\n",
    "    idx = np.random.choice(X_train_rand.shape[0], 1, replace=False)\n",
    "    \n",
    "    # add instance to training set\n",
    "    if t == 0:\n",
    "        X_rand = X_train_rand[idx,:]\n",
    "        Y_rand = y_train_rand[idx]\n",
    "    else:\n",
    "        new_x = X_train_rand[idx,:]\n",
    "        X_rand = np.append(X_rand,new_x,axis=0)\n",
    "        new_y = y_train_rand[idx]\n",
    "        Y_rand = np.append(Y_rand,new_y)\n",
    "\n",
    "\n",
    "    if 0 in Y_rand and 1 in Y_rand:\n",
    "\n",
    "        # train model and score it\n",
    "        clf = get_classifier('Random Forest')\n",
    "        clf.fit(X_rand, Y_rand)\n",
    "        score = 1-clf.score(X_test, y_test)\n",
    "        random_error.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append values to front to make up for lack of training at the start\n",
    "diff = BUDGET - len(random_error)\n",
    "if diff > 0:\n",
    "    random_error = [None for i in range(diff)] + random_error\n",
    "\n",
    "# plot all results together\n",
    "axs = plt.subplot()\n",
    "for i, loss in enumerate(losses):\n",
    "    axs.plot(np.arange(len(loss)), loss, label=\"Active learning \" + str(i))\n",
    "\n",
    "axs.plot(np.arange(len(random_error)), np.array(random_error), label=\"Offline learning RF\")\n",
    "plt.title('Number of Queries vs Error Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"img/q2_2_offline.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-harvey",
   "metadata": {},
   "source": [
    "## Part 2.3 Questions (10 points):\n",
    "**Answer the following questions.**\n",
    "### What is an \"admissible pair\" according to the paper (5 points)?\n",
    "\n",
    "According to the paper, an admissible pair $(v,l)$ is an admissible label $l$ for given node $v$. A label $l$ is admissible if assigning that label to a given node results in at most $\\beta$ times as much error as assigning any other label to that node. \n",
    "\n",
    "Maintaining a set of admissible pairs for all nodes for a given time step $t$ prevents DH from descending too far into the tree when the statistics it is using to make decisions are sparse.\n",
    "\n",
    "\n",
    "### Please explain the sampling bias that is dealt with in the DH algorithm and why it would be a problem if we just query the unlabeled point which is closest to the decision boundary (5 points)?\n",
    "\n",
    "\n",
    "#### Part 1 - Explain how sampling bias is handled\n",
    "Sampling bias is introduced when active learning algorithms focus queries in targeted regions of the sample space rather than drawing i.i.d. from the sampling distribution. This results in the training set having a different distribution than the test set, and bias can lead to poor generalization.\n",
    "\n",
    "With DH, sampling bias is handled in two parts, following the two parts of selecting nodes. \n",
    "\n",
    "First, select a subtree with a probability weight proportional to their size within the larger tree. For the test distribution, the proportion of i.i.d. samples that come from a particular subtree should follow the proportion of the subtree to the tree (i.e. a small region of the sample space is likely to be sampled less than a large region of the sample space). Setting probability weights using this proportion corrects for sampling bias in the training distribution. \n",
    "\n",
    "Each subtree has a size greater than zero, so there is always a chance of selecting nodes from that subtree in the current pruning. \n",
    "\n",
    "Once a subtree has been chosen, select points within that subtree uniformly at random. Each node in the subtree has the same chance of being chosen.\n",
    "\n",
    "\n",
    "\n",
    "#### Part 2 - Explain why querying the boundary would be a problem\n",
    "One strategy for active learning query selection is to select unlabeled points closest to the decision boundary (current estimate). If the sampling space is piecewise, information is concentrated in the boundary between pieces. By focusing a limited query budget on the boundary, the algorithm can efficiently learn this information and produce an accurate model. \n",
    "\n",
    "The problem with focusing on the currently known decision boundary is that we might never discover regions of difference in other parts of the sample space. (A part of the sample space believed to be uniform may turn out to have distinct classes within it.) The consequence is that our model would not generalize as well as it possibly could, had it found this region of difference, so we are left with a suboptimal model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
